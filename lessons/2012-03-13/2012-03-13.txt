% Polyhedral geometry 1 
% Computational Visual Design ([CVD-Lab](https://github.com/cvd-lab)), DIA, "Roma Tre" University, Rome, Italy 
% Computational Graphics 2012


# Linear spaces

## Definition

A **linear** (or *vector*) **space** ${\cal V}$ over a field ${\cal F}$ is a set with two composition rules, such that, for each $\p{u},\p{v},\p{w}\in{\cal V}$ and for each $\alpha,\beta\in {\cal F}$, the rules $+, \cdot$  satisfy the following axioms:


#. $\p{v}+\p{w} = \p{w}+\p{v}$; $\hfill$ (commutativity of addition)

#. $\p{u}+(\p{v}+\p{w}) = (\p{u}+\p{v})+\p{w}$; $\hfill$ (associativity of addition)

#. there is a $\p{0}\in{\cal V}$ such that $\p{v}+\p{0}=\p{v}$; $\hfill$ (neutral el. of addition)

#. there is a $-\p{v}\in{\cal V}$ such that $\p{v}+(-\p{v})=\p{0}$; $\hfill$ (inverse of add.)

#. $\alpha\cdot(\p{v}+\p{w}) = \alpha\cdot \p{v} + \alpha\cdot \p{w}$; $\hfill$ (distrib. of addition w.r.t. product)

#. $(\alpha+\beta)\cdot \p{v} = \alpha\cdot \p{v} + \alpha\cdot \p{v}$; $\hfill$ (distrib. of product w.r.t. addition)

#. $\alpha\cdot(\beta\cdot \p{v}) = (\alpha \beta) \cdot \p{v}$; $\hfill$ (associativity of product)

#. $1 \cdot \p{v} = \p{v}$. $\hfill$ (neutral element of product)



## Example: vector space of real matrices

Let ${\cal M}^m_n(\R)$ be the set of $m\times n$ matrices with elements in the field $\R$.  An element $A$ in such a set is denoted as $$A=(\alpha_{ij})$$

**Addition** and **multiplication by a scalar** are defined component-wise: $$ A+B=(\alpha_{ij})+(\beta_{ij})=(\alpha_{ij}+\beta_{ij}) $$ $$ \gamma A=\gamma(\alpha_{ij})=(\gamma\alpha_{ij}) $$


## Example: vector space of polynomials of degree $\leq n$

A linear space we will make often use of in *Computer Graphics* and *Geometric modeling* 
is the space of dimension $n+1$:
$$
{\cal P}^n(\R) = \{ p: \R \rightarrow \R: u \mapsto \sum_{i=1}^n a_i p^i , a_i\in\R\}
$$
of univariate **polynomials of degree $\leq n$** on the real field (with real coefficients),
with $p^i \in P_n$, where
$$
P_n = (p^n, p^{n-1}, ..., p^1, p^0)  \quad\mbox{and}\quad p^i: u \mapsto u^i 
$$
is **the power basis**.

## Subspace

Let $({\cal V},+,\cdot)$ be a vector space on the field ${\cal F}$.

${\cal U}\subset{\cal V}$ is a **subspace** of ${\cal V}$ 
:	if $({\cal U},+,\cdot)$ is a vector space with respect to the same operations.

${\cal U}\subset{\cal V}$ is a \emph{subspace} of ${\cal V}$ 

:	if and only if
:	${\cal U}\neq\emptyset$; 
:	for each $\alpha\in {\cal F}$ and $\v{u}_1,\v{u}_2\in{\cal U}$, $\alpha \v{u}_1+\v{u}_2 \in{\cal U}$

codimension
:	of a subspace ${\cal U}\subset{\cal V}$ 
:	is defined as $$\dim{\cal V} - \dim{\cal U}$$

Examples of codimension 
:	in 1D, 2D, 3D?


## Linear combination


Let $\v{v}_1,\v{v}_2,\ldots,\v{v}_n\in{\cal V}$ and $\alpha_1,\alpha_2, \ldots,\alpha_n\in {\cal F}$,


The vector $$ \alpha_1 \v{v}_1 + \cdots + \alpha_n \v{v}_n = \sum_{i=1}^n \alpha_i \v{v}_i \in{\cal V} $$ is called a **linear combination** of vectors $\v{v}_1,\v{v}_2,\ldots,\v{v}_n$ with scalars $\alpha_1,\alpha_2,\ldots,\alpha_n\in {\cal F}$


## Span

* The set of all linear combinations of elements of a set $S \subset {\cal V}$ is a subspace of ${\cal V}$.

* Such a subspace is called the **span of $S$** and is denoted as $$ \lin S $$

* If a subspace ${\cal U}$ of ${\cal V}$ can be generated as the span of a set $S$ of vectors in ${\cal V}$, then $S$ is called a \emph{generating set} or a \emph{spanning set} for ${\cal U}$.


## Linear independence

* A set of vectors $\{\v{v}_1,\v{v}_2,\ldots,\v{v}_n\}$ is **linearly independent** if $$ \sum_{i=1}^n \alpha_i \v{v}_i = \v{0} $$ implies that $\alpha_i=0$ for each $i$

* As a consequence, *a set of vectors is linearly independent* when none of them belongs to the span of the others.


## Bases and coordinates

When working with vector spaces, the concept of **basis**, a
*discrete subset of linearly independent elements*, is probably the most
useful to deal with.  

* each element of the space can be
*represented **uniquely** as linear combination of basis elements*  

* this
leads to a **parametrization** of the space, i.e. to *represent
each element by a sequence of scalars*, called its
**coordinates** with respect to the chosen basis.

## Bases

A set of vectors $\{\v{e}_1,\v{e}_2,\ldots,\v{e}_n\}$ is a **basis** for
the vector space ${\cal V}$ iff

#. the set is linearly independent, and
#. ${\cal V} = \lin \{ \v{e}_1,\v{e}_2,\ldots,\v{e}_n \}$


## Bases

*	Every two bases of ${\cal V}$ have the same number  of elements, that is called the *dimension* of  ${\cal V}$ and is denoted $$ \dim {\cal V} $$

*	Some important **properties** of the bases of a vector space are:

	#.  each spanning set for  ${\cal V}$ contains a basis;
	#.  each minimal spanning set is a basis;
	#.  each linearly independent set of vectors is contained in a basis;
	#.  each maximal set of linearly independent vectors is a basis;


## Components

If $(\v{e}_1,\v{e}_2,\ldots,\v{e}_n)$ is an ordered basis for ${\cal V}$,
then for each $\v{v}\in{\cal V}$ there exists a **unique** $n$-tuple of
scalars $\alpha_1,\alpha_2,\ldots, \alpha_n \in {\cal F}$ such that
$$
\v{v}=\sum_{i=1}^n \alpha_i \v{e}_i.
$$

## Components

The $n$-tuple of scalars $(\alpha_i)$ is called the
**components** of $\v{v}$ with respect to the
ordered basis $(\v{e}_1,\v{e}_2,\ldots,\v{e}_n)$.  

* If such a $n$-tuple
were not unique, then $\v{v}=\sum \alpha_i \v{e}_i=\sum \beta_i \v{e}_i$ 

* But this one would imply $\sum(\alpha_i-\beta_i) \v{e}_i
=\v{0}$, hence $(\alpha_i-\beta_i)=0$, 

* i.e. $\alpha_i=\beta_i$, for every $i$.

## Change of basis

* Let $B = (\v{e}_1, \ldots, \v{e}_n)\subset\cal V$ be a basis for $\cal V$. 

* Of course, their coordinates are $\vet{1 & 0 & \cdots & 0}, \vet{0 & 1 & \cdots & 0}, \ldots, \vet{0 & 0 & \cdots & 1}$, and, in $B$ coordinates, the basis is represented by the matrix 
$$ [B] = [I] $$.

* If we take $n$  (linearly independent) vectors $V = (\v{v}_1, \ldots, \v{v}_n) \subset\cal V$, represented in $B$ coordinates as $[V]$, and want to parametrize $\cal V$ with respect to the new basis, we have, for transformation of coordinates:
$$
[I] = [T][V]
$$

* and hence:
$$
[T] = [V]^{-1}
$$


## Example: two polynomial bases

* Let $P_3 = (u^3,u^2,u,1)$ 

* and $B_3 = ((1-u)^3, 3u(1-u)^2, 3u^2(1-u), u^3)$ be two ordered bases 

* for the linear space ${\cal P}^3(\R)$ of polynomials with  $\deg \leq 3$.

* the $[B_3]$ matrix in the $P_3$ basis is 
$$ [B_3]_{P_3} = 
\mat{-1 & 3 & -3 & 1 \\
	3 & -6 & 3 & 0 \\
	-3 & 3 & 0 & 0 \\
	1 & 0 & 0 & 0} $$

* the $[P_3]$ matrix in the $B_3$ basis is 
$$ [P_3]_{B_3} =  [B_3]_{P_3}^{-1} = 
\mat{0 & 0 & 0 & 1 \\
	0 & 0 & 1/3 & 1 \\
	0 & 1/3 & 1/6 & 1 \\
	1 & 1 & 1 & 1} $$
	
* **WHY ?**

